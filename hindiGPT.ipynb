{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annwhocodes/HindiTokenizerExtension/blob/main/hindiGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTwyP9aRHM4C"
      },
      "source": [
        "# Hindi Tokenizer Training Notebook\n",
        "\n",
        "This notebook implements a Hindi tokenizer using a transformer-based model architecture. The tokenizer is trained on 10 Hindi Wikipedia pages for 5 epochs, with proper layered architecture and vocabulary storage.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. Data Collection: Collect 10 Hindi Wikipedia pages\n",
        "2. Data Preprocessing: Clean and prepare the Hindi text data\n",
        "3. Model Architecture: Define a transformer-based tokenizer model\n",
        "4. Training: Train the model for 5 epochs\n",
        "5. Vocabulary Analysis: Analyze and visualize the learned vocabulary\n",
        "6. Testing: Test the tokenizer on new Hindi text\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhJIldSBHM4D"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, we install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_MC7V0gVTk3"
      },
      "outputs": [],
      "source": [
        "!pip install nbstripout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6iKIEs0WpUg"
      },
      "outputs": [],
      "source": [
        "!find /content -name \"hindiGPT.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRB9pxXcVdPV"
      },
      "outputs": [],
      "source": [
        "!nbstripout hindiGPT.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQOImKA2HM4E"
      },
      "outputs": [],
      "source": [
        "!pip install torch matplotlib pandas seaborn scikit-learn beautifulsoup4 tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD1RL3kQbiOT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kwEXkxdHM4E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_tF4P94cSz1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvZ1cRbWcZ_T"
      },
      "outputs": [],
      "source": [
        "os.makedirs('hindi_pages', exist_ok=True)\n",
        "os.makedirs('model_output', exist_ok=True)\n",
        "os.makedirs('vocab_analysis', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_4bQvApHM4F"
      },
      "source": [
        "## 1. Data Collection\n",
        "\n",
        "Let's collect 10 Hindi Wikipedia pages for training our tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV_GiCvPHM4F"
      },
      "outputs": [],
      "source": [
        "def get_hindi_wikipedia_page(page_title=None):\n",
        "    \"\"\"\n",
        "    Download a specific Hindi Wikipedia page or a random one if no title provided\n",
        "\n",
        "    Args:\n",
        "        page_title: Optional title of the Wikipedia page to download\n",
        "\n",
        "    Returns:\n",
        "        tuple: (title, text) of the downloaded page\n",
        "    \"\"\"\n",
        "    print(f\"Downloading Hindi Wikipedia page: {page_title if page_title else 'Random page'}\")\n",
        "\n",
        "    if page_title:\n",
        "        page_title = page_title.replace(' ', '_')\n",
        "\n",
        "        import urllib.parse\n",
        "        page_title = urllib.parse.quote(page_title)\n",
        "        hindi_wiki_url = f\"https://hi.wikipedia.org/wiki/{page_title}\"\n",
        "    else:\n",
        "        hindi_wiki_url = \"https://hi.wikipedia.org/wiki/विशेष:यादृच्छिक\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'hi,en;q=0.9'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(hindi_wiki_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title_element = soup.find('h1', {'id': 'firstHeading'})\n",
        "        if title_element:\n",
        "            title = title_element.text.strip()\n",
        "        else:\n",
        "            title = \"Unknown Title\"\n",
        "\n",
        "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
        "        if not content_div:\n",
        "            return title, \"Could not extract content\"\n",
        "\n",
        "        # Get all paragraphs from the content\n",
        "        paragraphs = content_div.find_all('p')\n",
        "        text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Clean up the text\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation numbers like [1], [2], etc.\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "\n",
        "        if len(text) > 100:\n",
        "            return title, text\n",
        "        else:\n",
        "            print(f\"Content too short for page: {title}\")\n",
        "            return title, \"Content too short\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading Wikipedia page: {e}\")\n",
        "        return \"Error\", f\"Error: {str(e)}\"\n",
        "\n",
        "def collect_hindi_wikipedia_pages(num_pages=10, output_dir=\"hindi_pages\", specific_pages=None):\n",
        "    \"\"\"\n",
        "    Collect multiple Hindi Wikipedia pages and save them to files\n",
        "\n",
        "    Args:\n",
        "        num_pages: Number of pages to collect\n",
        "        output_dir: Directory to save the pages\n",
        "        specific_pages: Optional list of specific page titles to download\n",
        "\n",
        "    Returns:\n",
        "        list: Paths to the saved files\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    popular_hindi_pages = [\n",
        "        \"भारत\",  # India\n",
        "        \"हिन्दी\",  # Hindi\n",
        "        \"दिल्ली\",  # Delhi\n",
        "        \"मुंबई\",  # Mumbai\n",
        "        \"महात्मा_गांधी\",  # Mahatma Gandhi\n",
        "        \"बॉलीवुड\",  # Bollywood\n",
        "        \"क्रिकेट\",  # Cricket\n",
        "        \"हिमालय\",  # Himalaya\n",
        "        \"योग\",  # Yoga\n",
        "        \"आयुर्वेद\",  # Ayurveda\n",
        "        \"रामायण\",  # Ramayana\n",
        "        \"महाभारत\",  # Mahabharata\n",
        "        \"ताज_महल\",  # Taj Mahal\n",
        "        \"जवाहरलाल_नेहरू\",  # Jawaharlal Nehru\n",
        "        \"अमिताभ_बच्चन\",  # Amitabh Bachchan\n",
        "    ]\n",
        "\n",
        "    saved_files = []\n",
        "    collected_pages = 0\n",
        "\n",
        "\n",
        "    if specific_pages:\n",
        "        for page_title in tqdm(specific_pages, desc=\"Collecting specific pages\"):\n",
        "            if collected_pages >= num_pages:\n",
        "                break\n",
        "\n",
        "            title, content = get_hindi_wikipedia_page(page_title)\n",
        "\n",
        "            if content and len(content) > 500 and content != \"Content too short\" and not content.startswith(\"Error\"):\n",
        "                # Save the page content\n",
        "                safe_title = re.sub(r'[^\\w]', '_', title)\n",
        "                filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}.txt\"\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                # Save metadata\n",
        "                metadata_filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}_meta.json\"\n",
        "                with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "                    json.dump({\n",
        "                        \"title\": title,\n",
        "                        \"url\": f\"https://hi.wikipedia.org/wiki/{page_title.replace(' ', '_')}\",\n",
        "                        \"length\": len(content),\n",
        "                        \"date_collected\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                saved_files.append(filename)\n",
        "                collected_pages += 1\n",
        "                print(f\"Saved page {collected_pages}: {title}\")\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "    attempts = 0\n",
        "    while collected_pages < num_pages and attempts < 20:\n",
        "        title, content = get_hindi_wikipedia_page()\n",
        "\n",
        "        if content and len(content) > 500 and content != \"Content too short\" and not content.startswith(\"Error\"):\n",
        "            safe_title = re.sub(r'[^\\w]', '_', title)\n",
        "            filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}.txt\"\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            # Save metadata\n",
        "            metadata_filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}_meta.json\"\n",
        "            with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    \"title\": title,\n",
        "                    \"url\": \"Random page\",\n",
        "                    \"length\": len(content),\n",
        "                    \"date_collected\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            saved_files.append(filename)\n",
        "            collected_pages += 1\n",
        "            print(f\"Saved page {collected_pages}: {title}\")\n",
        "        else:\n",
        "            attempts += 1\n",
        "\n",
        "        # Add a small delay to avoid hitting rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    # If we still don't have enough pages, use the popular pages list\n",
        "    if collected_pages < num_pages:\n",
        "        for page_title in popular_hindi_pages:\n",
        "            if collected_pages >= num_pages:\n",
        "                break\n",
        "\n",
        "            title, content = get_hindi_wikipedia_page(page_title)\n",
        "\n",
        "            if content and len(content) > 500 and content != \"Content too short\" and not content.startswith(\"Error\"):\n",
        "                # Save the page content\n",
        "                safe_title = re.sub(r'[^\\w]', '_', title)\n",
        "                filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}.txt\"\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                # Save metadata\n",
        "                metadata_filename = f\"{output_dir}/page_{collected_pages+1}_{safe_title}_meta.json\"\n",
        "                with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "                    json.dump({\n",
        "                        \"title\": title,\n",
        "                        \"url\": f\"https://hi.wikipedia.org/wiki/{page_title}\",\n",
        "                        \"length\": len(content),\n",
        "                        \"date_collected\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                saved_files.append(filename)\n",
        "                collected_pages += 1\n",
        "                print(f\"Saved page {collected_pages}: {title}\")\n",
        "\n",
        "                # Add a small delay to avoid hitting rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "    print(f\"Collected {collected_pages} Hindi Wikipedia pages\")\n",
        "    return saved_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbTIVF_oHM4G"
      },
      "outputs": [],
      "source": [
        "# Define specific Hindi Wikipedia pages to ensure quality content\n",
        "specific_pages = [\n",
        "    \"भारत\",  # India\n",
        "    \"हिन्दी\",  # Hindi\n",
        "    \"दिल्ली\",  # Delhi\n",
        "    \"मुंबई\",  # Mumbai\n",
        "    \"महात्मा_गांधी\",  # Mahatma Gandhi\n",
        "    \"बॉलीवुड\",  # Bollywood\n",
        "    \"क्रिकेट\",  # Cricket\n",
        "    \"हिमालय\",  # Himalaya\n",
        "    \"योग\",  # Yoga\n",
        "    \"आयुर्वेद\",  # Ayurveda\n",
        "]\n",
        "\n",
        "# Collect 10 Hindi Wikipedia pages\n",
        "saved_files = collect_hindi_wikipedia_pages(\n",
        "    num_pages=10,\n",
        "    output_dir=\"hindi_pages\",\n",
        "    specific_pages=specific_pages\n",
        ")\n",
        "\n",
        "print(f\"\\nSaved {len(saved_files)} files:\")\n",
        "for file in saved_files:\n",
        "    print(f\"  - {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbX-mUOHM4G"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Now, let's preprocess the collected Hindi text data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CgHVwCPHM4H"
      },
      "outputs": [],
      "source": [
        "def preprocess_hindi_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess Hindi text data\n",
        "\n",
        "    Args:\n",
        "        text: Raw Hindi text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned and preprocessed text\n",
        "    \"\"\"\n",
        "    # Replace HTML entities\n",
        "    text = re.sub(r'&[a-zA-Z]+;', ' ', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "    # Remove citation numbers like [1], [2], etc.\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "\n",
        "    # Remove non-Devanagari characters except spaces and basic punctuation\n",
        "    # Devanagari Unicode range: \\u0900-\\u097F\n",
        "    text = re.sub(r'[^\\u0900-\\u097F\\s।,.?!-]', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove very short lines (likely headers or incomplete sentences)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line for line in lines if len(line.strip()) > 20]\n",
        "    text = '\\n'.join(filtered_lines)\n",
        "\n",
        "    return text\n",
        "\n",
        "def combine_hindi_pages(input_dir, output_file):\n",
        "    \"\"\"\n",
        "    Combine multiple Hindi text files into a single corpus file\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing the Hindi text files\n",
        "        output_file: Path to the output corpus file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (total_chars, total_words, num_files)\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "    num_files = 0\n",
        "\n",
        "    # Get all text files in the input directory\n",
        "    import glob\n",
        "    text_files = glob.glob(os.path.join(input_dir, \"*.txt\"))\n",
        "\n",
        "    # Filter out metadata files\n",
        "    text_files = [f for f in text_files if not f.endswith(\"_meta.json\")]\n",
        "\n",
        "    print(f\"Found {len(text_files)} text files to process\")\n",
        "\n",
        "    for file_path in tqdm(text_files, desc=\"Processing files\"):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # Skip metadata files\n",
        "            if file_path.endswith(\"_meta.json\"):\n",
        "                continue\n",
        "\n",
        "            # Preprocess the text\n",
        "            clean_text = preprocess_hindi_text(text)\n",
        "\n",
        "            # Add a separator between documents\n",
        "            if all_text:\n",
        "                all_text += \"\\n\\n\" + clean_text\n",
        "            else:\n",
        "                all_text = clean_text\n",
        "\n",
        "            num_files += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Write the combined text to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(all_text)\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_chars = len(all_text)\n",
        "    total_words = len(all_text.split())\n",
        "\n",
        "    print(f\"Combined {num_files} files into {output_file}\")\n",
        "    print(f\"Total characters: {total_chars}\")\n",
        "    print(f\"Total words (approx): {total_words}\")\n",
        "\n",
        "    return total_chars, total_words, num_files\n",
        "\n",
        "def analyze_hindi_corpus(corpus_file):\n",
        "    \"\"\"\n",
        "    Analyze the Hindi corpus and print statistics\n",
        "\n",
        "    Args:\n",
        "        corpus_file: Path to the corpus file\n",
        "\n",
        "    Returns:\n",
        "        dict: Statistics about the corpus\n",
        "    \"\"\"\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Calculate basic statistics\n",
        "    total_chars = len(text)\n",
        "    total_words = len(text.split())\n",
        "    unique_words = len(set(text.split()))\n",
        "\n",
        "    # Count Devanagari characters\n",
        "    devanagari_chars = re.findall(r'[\\u0900-\\u097F]', text)\n",
        "    unique_devanagari = len(set(devanagari_chars))\n",
        "\n",
        "    # Count punctuation\n",
        "    punctuation = re.findall(r'[।,.?!-]', text)\n",
        "\n",
        "    stats = {\n",
        "        \"total_characters\": total_chars,\n",
        "        \"total_words\": total_words,\n",
        "        \"unique_words\": unique_words,\n",
        "        \"devanagari_characters\": len(devanagari_chars),\n",
        "        \"unique_devanagari\": unique_devanagari,\n",
        "        \"punctuation_marks\": len(punctuation)\n",
        "    }\n",
        "\n",
        "    print(\"\\nCorpus Statistics:\")\n",
        "    print(f\"Total characters: {stats['total_characters']}\")\n",
        "    print(f\"Total words (approx): {stats['total_words']}\")\n",
        "    print(f\"Unique words (approx): {stats['unique_words']}\")\n",
        "    print(f\"Devanagari characters: {stats['devanagari_characters']}\")\n",
        "    print(f\"Unique Devanagari characters: {stats['unique_devanagari']}\")\n",
        "    print(f\"Punctuation marks: {stats['punctuation_marks']}\")\n",
        "\n",
        "    # Save statistics to a JSON file\n",
        "    stats_file = corpus_file.replace('.txt', '_stats.json')\n",
        "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Statistics saved to {stats_file}\")\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUEcspFwHM4H"
      },
      "outputs": [],
      "source": [
        "# Combine all Hindi Wikipedia pages into a single corpus file\n",
        "input_dir = \"hindi_pages\"\n",
        "output_file = \"/content/drive/MyDrive/hindi_corpus.txt\"\n",
        "\n",
        "total_chars, total_words, num_files = combine_hindi_pages(input_dir, output_file)\n",
        "\n",
        "# Analyze the corpus\n",
        "corpus_stats = analyze_hindi_corpus(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NX5U2PMHM4H"
      },
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "Now, let's define our Hindi tokenizer model architecture with the fixed attention mask handling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3i5gLqUHM4I"
      },
      "outputs": [],
      "source": [
        "class SimpleHindiTokenizer:\n",
        "    \"\"\"\n",
        "    A lightweight Hindi tokenizer specifically designed for use with transformer models.\n",
        "    This tokenizer handles Hindi text (Devanagari script) and provides methods for\n",
        "    encoding and decoding text, as well as vocabulary management.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.inverse_vocab = {}\n",
        "        self.word_freq = Counter()\n",
        "        self.special_tokens = {\n",
        "            \"[PAD]\": 0,\n",
        "            \"[UNK]\": 1,\n",
        "            \"[CLS]\": 2,\n",
        "            \"[SEP]\": 3,\n",
        "            \"[MASK]\": 4,\n",
        "        }\n",
        "\n",
        "    def preprocess_hindi_text(self, text):\n",
        "        \"\"\"Clean and preprocess Hindi text data\"\"\"\n",
        "        # Replace HTML entities\n",
        "        text = re.sub(r'&[a-zA-Z]+;', ' ', text)\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "        # Remove citation numbers like [1], [2], etc.\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "\n",
        "        # Keep only Hindi characters, spaces, and punctuation\n",
        "        text = re.sub(r'[^\\u0900-\\u097F\\s।,.?!-]', '', text)\n",
        "\n",
        "        # Clean up whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def simple_word_tokenize(self, text):\n",
        "        \"\"\"Simple word tokenizer for Hindi that splits on spaces and punctuation\"\"\"\n",
        "        # First replace punctuation with spaces\n",
        "        text = re.sub(r'[।,.?!]', ' ', text)\n",
        "        # Split on whitespace\n",
        "        words = text.split()\n",
        "        return words\n",
        "\n",
        "    def build_vocabulary(self, text, max_vocab_size=5000):\n",
        "        \"\"\"Build a vocabulary from the processed text\"\"\"\n",
        "        print(\"Building vocabulary...\")\n",
        "        vocab = {token: idx for token, idx in self.special_tokens.items()}\n",
        "        current_idx = len(vocab)\n",
        "\n",
        "        words = self.simple_word_tokenize(text)\n",
        "        self.word_freq.update(words)\n",
        "\n",
        "        # Print some stats about the vocabulary\n",
        "        total_words = len(words)\n",
        "        unique_words = len(set(words))\n",
        "        print(f\"Total words in corpus: {total_words}\")\n",
        "        print(f\"Unique words in corpus: {unique_words}\")\n",
        "\n",
        "        # Print most common words\n",
        "        print(\"Most common words:\")\n",
        "        for word, count in self.word_freq.most_common(10):\n",
        "            print(f\" {word}: {count}\")\n",
        "\n",
        "        for word, _ in self.word_freq.most_common(max_vocab_size - len(vocab)):\n",
        "            vocab[word] = current_idx\n",
        "            current_idx += 1\n",
        "\n",
        "        inverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "        self.vocab = vocab\n",
        "        self.inverse_vocab = inverse_vocab\n",
        "\n",
        "        print(f\"Vocabulary built with {len(vocab)} tokens\")\n",
        "        return vocab, inverse_vocab\n",
        "\n",
        "    def save_tokenizer(self, filepath=\"hindi_tokenizer.json\"):\n",
        "        \"\"\"Save the tokenizer vocabulary to a JSON file\"\"\"\n",
        "        data = {\n",
        "            \"vocab\": self.vocab,\n",
        "            \"special_tokens\": self.special_tokens\n",
        "        }\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Tokenizer saved to {filepath}\")\n",
        "\n",
        "    def load_tokenizer(self, filepath=\"hindi_tokenizer.json\"):\n",
        "        \"\"\"Load a previously saved tokenizer\"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        self.vocab = data[\"vocab\"]\n",
        "        self.special_tokens = data[\"special_tokens\"]\n",
        "        self.inverse_vocab = {int(idx): token for token, idx in self.vocab.items()}\n",
        "        print(f\"Loaded tokenizer with {len(self.vocab)} tokens\")\n",
        "\n",
        "    def encode(self, text, add_special_tokens=False):\n",
        "        \"\"\"Encode text into token IDs\"\"\"\n",
        "        words = self.simple_word_tokenize(text)\n",
        "        token_ids = []\n",
        "        tokens = []\n",
        "\n",
        "        # Add [CLS] token at the beginning if requested\n",
        "        if add_special_tokens:\n",
        "            token_ids.append(self.special_tokens[\"[CLS]\"])\n",
        "            tokens.append(\"[CLS]\")\n",
        "\n",
        "        # Encode each word\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                token_ids.append(self.vocab[word])\n",
        "                tokens.append(word)\n",
        "            else:\n",
        "                token_ids.append(self.special_tokens[\"[UNK]\"])\n",
        "                tokens.append(\"[UNK]\")\n",
        "\n",
        "        # Add [SEP] token at the end if requested\n",
        "        if add_special_tokens:\n",
        "            token_ids.append(self.special_tokens[\"[SEP]\"])\n",
        "            tokens.append(\"[SEP]\")\n",
        "\n",
        "        return {\"ids\": token_ids, \"tokens\": tokens}\n",
        "\n",
        "    def decode(self, token_ids, skip_special_tokens=True):\n",
        "        \"\"\"Decode token IDs back to text\"\"\"\n",
        "        words = []\n",
        "        for idx in token_ids:\n",
        "            # Skip special tokens if requested\n",
        "            if skip_special_tokens and idx in [\n",
        "                self.special_tokens[\"[PAD]\"],\n",
        "                self.special_tokens[\"[CLS]\"],\n",
        "                self.special_tokens[\"[SEP]\"],\n",
        "                self.special_tokens[\"[MASK]\"]\n",
        "            ]:\n",
        "                continue\n",
        "\n",
        "            if idx in self.inverse_vocab:\n",
        "                words.append(self.inverse_vocab[idx])\n",
        "            else:\n",
        "                words.append(\"[UNK]\")\n",
        "\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def encode_batch(self, texts, max_length=None, padding=False, truncation=False):\n",
        "        \"\"\"\n",
        "        Encode a batch of texts\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts to encode\n",
        "            max_length: Maximum sequence length\n",
        "            padding: Whether to pad sequences to max_length\n",
        "            truncation: Whether to truncate sequences to max_length\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with encoded IDs and attention masks\n",
        "        \"\"\"\n",
        "        encoded_batch = {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "        for text in texts:\n",
        "            encoded = self.encode(text)\n",
        "            ids = encoded[\"ids\"]\n",
        "\n",
        "            # Truncate if needed\n",
        "            if truncation and max_length and len(ids) > max_length:\n",
        "                ids = ids[:max_length]\n",
        "\n",
        "            # Create attention mask (1 for real tokens, 0 for padding)\n",
        "            attention_mask = [1] * len(ids)\n",
        "\n",
        "            # Pad if needed\n",
        "            if padding and max_length:\n",
        "                padding_length = max_length - len(ids)\n",
        "                ids = ids + [self.special_tokens[\"[PAD]\"]] * padding_length\n",
        "                attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "            encoded_batch[\"input_ids\"].append(ids)\n",
        "            encoded_batch[\"attention_mask\"].append(attention_mask)\n",
        "\n",
        "        return encoded_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vA4hjICHM4I"
      },
      "outputs": [],
      "source": [
        "class HindiTextDataset(Dataset):\n",
        "    \"\"\"Dataset for Hindi text data\"\"\"\n",
        "    def __init__(self, text, tokenizer, seq_length=64):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Tokenize the text\n",
        "        words = tokenizer.simple_word_tokenize(text)\n",
        "\n",
        "        # Convert words to token IDs\n",
        "        self.token_ids = []\n",
        "        for word in words:\n",
        "            if word in tokenizer.vocab:\n",
        "                self.token_ids.append(tokenizer.vocab[word])\n",
        "            else:\n",
        "                self.token_ids.append(tokenizer.special_tokens[\"[UNK]\"])\n",
        "\n",
        "        # Calculate the number of sequences\n",
        "        self.num_sequences = max(1, len(self.token_ids) - seq_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sequence of token IDs\n",
        "        input_ids = self.token_ids[idx:idx + self.seq_length]\n",
        "\n",
        "        # Pad if necessary\n",
        "        if len(input_ids) < self.seq_length:\n",
        "            input_ids = input_ids + [self.tokenizer.special_tokens[\"[PAD]\"]] * (self.seq_length - len(input_ids))\n",
        "\n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids),\n",
        "            \"attention_mask\": torch.tensor(attention_mask)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp_wrYCwHM4I"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention mechanism with fixed attention mask handling\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_len = query.size(1)\n",
        "\n",
        "        # Linear projections and reshape for multi-head attention\n",
        "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attn_mask is not None:\n",
        "            # Properly reshape the attention mask to match the scores dimensions\n",
        "            # scores shape: [batch_size, num_heads, seq_len, seq_len]\n",
        "            # attn_mask shape: [batch_size, seq_len]\n",
        "            # We need to reshape attn_mask to [batch_size, 1, 1, seq_len]\n",
        "            # and then broadcast it to match scores\n",
        "            attn_mask = attn_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
        "\n",
        "        # Apply softmax and dropout\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and apply output projection\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network with residual connection\"\"\"\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First linear layer with GELU activation\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        # Dropout\n",
        "        x = self.dropout(x)\n",
        "        # Second linear layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Transformer encoder layer with multi-head attention and feed-forward network\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.self_attn(x, x, x, attn_mask)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class HindiTokenizerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Hindi tokenizer model with transformer architecture.\n",
        "    This model is designed for training a tokenizer on Hindi text.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=4, ff_dim=512, dropout=0.1, max_seq_length=64):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Token embeddings\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Position embeddings\n",
        "        self.position_embeddings = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_length = input_ids.size()\n",
        "\n",
        "        # Create position IDs\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        token_embeds = self.token_embeddings(input_ids)\n",
        "        position_embeds = self.position_embeddings(position_ids)\n",
        "\n",
        "        # Combine embeddings\n",
        "        embeddings = token_embeds + position_embeds\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # Apply transformer encoder layers\n",
        "        hidden_states = embeddings\n",
        "        for layer in self.encoder_layers:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # Apply output projection\n",
        "        logits = self.output_projection(hidden_states)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, input_ids, max_length=20, temperature=1.0):\n",
        "        \"\"\"Generate text from the model\"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # Forward pass\n",
        "                logits = self(input_ids)\n",
        "\n",
        "                # Get the logits for the last token\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # Sample from the distribution\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append the new token to the input\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        return input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xamZ32JKHM4J"
      },
      "source": [
        "## 4. Training Pipeline\n",
        "\n",
        "Now, let's set up the training pipeline for our Hindi tokenizer model with the fixed attention mask handling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNRr8Cn8HM4J"
      },
      "outputs": [],
      "source": [
        "def train_hindi_tokenizer(\n",
        "    corpus_file,\n",
        "    output_dir=\"model_output\",\n",
        "    vocab_size=5000,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    num_layers=4,\n",
        "    ff_dim=512,\n",
        "    dropout=0.1,\n",
        "    batch_size=32,\n",
        "    seq_length=64,\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.001,\n",
        "    save_every=10,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a Hindi tokenizer model\n",
        "\n",
        "    Args:\n",
        "        corpus_file: Path to the Hindi corpus file\n",
        "        output_dir: Directory to save model checkpoints and logs\n",
        "        vocab_size: Maximum vocabulary size\n",
        "        embed_dim: Embedding dimension\n",
        "        num_heads: Number of attention heads\n",
        "        num_layers: Number of transformer layers\n",
        "        ff_dim: Feed-forward dimension\n",
        "        dropout: Dropout rate\n",
        "        batch_size: Batch size for training\n",
        "        seq_length: Maximum sequence length\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        save_every: Save model checkpoint every N epochs\n",
        "        device: Device to use for training (cpu or cuda)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (tokenizer, model, training_stats)\n",
        "    \"\"\"\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Determine device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load corpus\n",
        "    print(f\"Loading corpus from {corpus_file}\")\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"Initializing tokenizer\")\n",
        "    tokenizer = SimpleHindiTokenizer()\n",
        "\n",
        "    # Build vocabulary\n",
        "    print(f\"Building vocabulary with max size {vocab_size}\")\n",
        "    tokenizer.build_vocabulary(corpus_text, max_vocab_size=vocab_size)\n",
        "\n",
        "    # Save tokenizer\n",
        "    tokenizer_path = os.path.join(output_dir, \"hindi_tokenizer.json\")\n",
        "    tokenizer.save_tokenizer(tokenizer_path)\n",
        "\n",
        "    # Create dataset\n",
        "    print(f\"Creating dataset with sequence length {seq_length}\")\n",
        "    dataset = HindiTextDataset(corpus_text, tokenizer, seq_length=seq_length)\n",
        "\n",
        "    # Create data loader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model\")\n",
        "    model = HindiTokenizerModel(\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        ff_dim=ff_dim,\n",
        "        dropout=dropout,\n",
        "        max_seq_length=seq_length\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Print model architecture\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(f\"Vocabulary Size: {len(tokenizer.vocab)}\")\n",
        "    print(f\"Embedding Dimension: {embed_dim}\")\n",
        "    print(f\"Number of Attention Heads: {num_heads}\")\n",
        "    print(f\"Number of Transformer Layers: {num_layers}\")\n",
        "    print(f\"Feed-Forward Dimension: {ff_dim}\")\n",
        "    print(f\"Dropout Rate: {dropout}\")\n",
        "    print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.special_tokens[\"[PAD]\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs\")\n",
        "    training_stats = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Create a progress bar for epochs\n",
        "    epoch_pbar = tqdm(range(1, num_epochs + 1), desc=\"Training epochs\")\n",
        "\n",
        "    for epoch in epoch_pbar:\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Create a progress bar for batches\n",
        "        batch_pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}\", leave=False)\n",
        "\n",
        "        for batch_idx, batch in batch_pbar:\n",
        "            # Get inputs and targets\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Shift for language modeling: inputs are all tokens except last, targets are all tokens except first\n",
        "            inputs = input_ids[:, :-1]\n",
        "            targets = input_ids[:, 1:]\n",
        "            mask = attention_mask[:, :-1]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(inputs, mask)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            logits = logits.view(-1, logits.size(-1))  # This line is probably OK\n",
        "            targets = targets.reshape(-1)  # Changed from .view(-1) to .reshape(-1)\n",
        "\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update batch progress bar\n",
        "            batch_pbar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        # Update epoch progress bar\n",
        "        epoch_pbar.set_postfix({\"avg_loss\": avg_loss, \"time\": f\"{epoch_time:.2f}s\"})\n",
        "\n",
        "        # Save training stats\n",
        "        training_stats.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"avg_loss\": avg_loss,\n",
        "            \"epoch_time\": epoch_time\n",
        "        })\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if epoch % save_every == 0 or epoch == num_epochs:\n",
        "            checkpoint_path = os.path.join(output_dir, f\"hindi_tokenizer_model_epoch_{epoch}.pt\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "                \"vocab_size\": len(tokenizer.vocab),\n",
        "                \"embed_dim\": embed_dim,\n",
        "                \"num_heads\": num_heads,\n",
        "                \"num_layers\": num_layers,\n",
        "                \"ff_dim\": ff_dim\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_model_path = os.path.join(output_dir, \"hindi_tokenizer_model_best.pt\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "                \"vocab_size\": len(tokenizer.vocab),\n",
        "                \"embed_dim\": embed_dim,\n",
        "                \"num_heads\": num_heads,\n",
        "                \"num_layers\": num_layers,\n",
        "                \"ff_dim\": ff_dim\n",
        "            }, best_model_path)\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = os.path.join(output_dir, \"hindi_tokenizer_model_final.pt\")\n",
        "    torch.save({\n",
        "        \"epoch\": num_epochs,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"loss\": avg_loss,\n",
        "        \"vocab_size\": len(tokenizer.vocab),\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"num_heads\": num_heads,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"ff_dim\": ff_dim\n",
        "    }, final_model_path)\n",
        "    print(f\"Final model saved to {final_model_path}\")\n",
        "\n",
        "    # Save training stats\n",
        "    stats_path = os.path.join(output_dir, \"training_stats.json\")\n",
        "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_stats, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Training stats saved to {stats_path}\")\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot([stat[\"epoch\"] for stat in training_stats], [stat[\"avg_loss\"] for stat in training_stats])\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Average Loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.grid(True)\n",
        "    loss_plot_path = os.path.join(output_dir, \"training_loss.png\")\n",
        "    plt.savefig(loss_plot_path)\n",
        "    plt.show()\n",
        "    print(f\"Training loss plot saved to {loss_plot_path}\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Final loss: {training_stats[-1]['avg_loss']:.4f}\")\n",
        "\n",
        "    return tokenizer, model, training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvQ6VbnHM4J"
      },
      "outputs": [],
      "source": [
        "def test_hindi_tokenizer(model, tokenizer, test_text=None):\n",
        "    \"\"\"\n",
        "    Test the trained Hindi tokenizer model\n",
        "\n",
        "    Args:\n",
        "        model: Trained HindiTokenizerModel\n",
        "        tokenizer: Trained SimpleHindiTokenizer\n",
        "        test_text: Optional test text to tokenize\n",
        "\n",
        "    Returns:\n",
        "        dict: Test results\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if test_text is None:\n",
        "        test_text = \"मेरा नाम जॉन है और मैं हिंदी सीख रहा हूँ।\"\n",
        "\n",
        "    print(f\"\\nTesting tokenizer with text: {test_text}\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    encoded = tokenizer.encode(test_text)\n",
        "    token_ids = encoded[\"ids\"]\n",
        "    tokens = encoded[\"tokens\"]\n",
        "\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "    # Decode the token IDs\n",
        "    decoded_text = tokenizer.decode(token_ids)\n",
        "    print(f\"Decoded text: {decoded_text}\")\n",
        "\n",
        "    # Test model generation\n",
        "    input_tensor = torch.tensor([token_ids[:5]], dtype=torch.long)\n",
        "    generated_ids = model.generate(input_tensor, max_length=10, temperature=0.8)\n",
        "    generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
        "    print(f\"Generated text from prefix: {generated_text}\")\n",
        "\n",
        "    return {\n",
        "        \"original_text\": test_text,\n",
        "        \"tokens\": tokens,\n",
        "        \"token_ids\": token_ids,\n",
        "        \"decoded_text\": decoded_text,\n",
        "        \"generated_text\": generated_text\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZj5MjK-HM4K"
      },
      "outputs": [],
      "source": [
        "tokenizer, model, stats = train_hindi_tokenizer(\n",
        "    corpus_file=\"/content/drive/MyDrive/hindi_corpus.txt\",\n",
        "    output_dir=\"/content/drive/MyDrive/MyHindiModelOutput\",\n",
        "    vocab_size=5000,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    num_layers=4,\n",
        "    ff_dim=512,\n",
        "    dropout=0.1,\n",
        "    batch_size=32,\n",
        "    seq_length=64,\n",
        "    num_epochs=5,\n",
        "    learning_rate=0.001,\n",
        "    save_every=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsKIrqBIHM4K"
      },
      "source": [
        "## 5. Vocabulary Analysis\n",
        "\n",
        "Let's analyze and visualize the vocabulary learned by our tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9brf-gEHM4K"
      },
      "outputs": [],
      "source": [
        "def load_tokenizer_vocabulary(tokenizer_file):\n",
        "    \"\"\"\n",
        "    Load and display the tokenizer vocabulary\n",
        "\n",
        "    Args:\n",
        "        tokenizer_file: Path to the tokenizer JSON file\n",
        "\n",
        "    Returns:\n",
        "        dict: Loaded vocabulary\n",
        "    \"\"\"\n",
        "    print(f\"Loading tokenizer vocabulary from {tokenizer_file}\")\n",
        "\n",
        "    with open(tokenizer_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    vocab = data[\"vocab\"]\n",
        "    special_tokens = data[\"special_tokens\"]\n",
        "\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "    print(f\"Special tokens: {special_tokens}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def analyze_vocabulary(vocab, output_dir=\"vocab_analysis\"):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the tokenizer vocabulary\n",
        "\n",
        "    Args:\n",
        "        vocab: Tokenizer vocabulary\n",
        "        output_dir: Directory to save visualizations\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Convert vocabulary to a list of (token, id) pairs\n",
        "    vocab_items = [(token, int(idx)) for token, idx in vocab.items()]\n",
        "\n",
        "    # Sort by token ID\n",
        "    vocab_items.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Extract tokens and IDs\n",
        "    tokens = [item[0] for item in vocab_items]\n",
        "    ids = [item[1] for item in vocab_items]\n",
        "\n",
        "    # Calculate token lengths\n",
        "    token_lengths = [len(token) for token in tokens]\n",
        "\n",
        "    # Count token length frequencies\n",
        "    length_counter = Counter(token_lengths)\n",
        "\n",
        "    # Prepare data for visualization\n",
        "    length_data = pd.DataFrame({\n",
        "        'Length': list(length_counter.keys()),\n",
        "        'Count': list(length_counter.values())\n",
        "    })\n",
        "\n",
        "    # Plot token length distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Length', y='Count', data=length_data)\n",
        "    plt.title('Token Length Distribution')\n",
        "    plt.xlabel('Token Length')\n",
        "    plt.ylabel('Count')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    length_plot_path = os.path.join(output_dir, \"token_length_distribution.png\")\n",
        "    plt.savefig(length_plot_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Save vocabulary to CSV for easy viewing\n",
        "    vocab_df = pd.DataFrame({\n",
        "        'Token': tokens,\n",
        "        'ID': ids,\n",
        "        'Length': token_lengths\n",
        "    })\n",
        "    vocab_csv_path = os.path.join(output_dir, \"vocabulary.csv\")\n",
        "    vocab_df.to_csv(vocab_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "    # Display the first 20 tokens\n",
        "    print(\"\\nFirst 20 tokens:\")\n",
        "    display(vocab_df.head(20))\n",
        "\n",
        "    # Calculate basic statistics\n",
        "    stats = {\n",
        "        \"vocabulary_size\": len(vocab),\n",
        "        \"avg_token_length\": sum(token_lengths) / len(token_lengths),\n",
        "        \"min_token_length\": min(token_lengths),\n",
        "        \"max_token_length\": max(token_lengths),\n",
        "        \"most_common_length\": length_counter.most_common(1)[0][0]\n",
        "    }\n",
        "\n",
        "    # Save statistics to JSON\n",
        "    stats_path = os.path.join(output_dir, \"vocabulary_stats.json\")\n",
        "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\nVocabulary Analysis:\")\n",
        "    print(f\"Vocabulary size: {stats['vocabulary_size']}\")\n",
        "    print(f\"Average token length: {stats['avg_token_length']:.2f}\")\n",
        "    print(f\"Min token length: {stats['min_token_length']}\")\n",
        "    print(f\"Max token length: {stats['max_token_length']}\")\n",
        "    print(f\"Most common token length: {stats['most_common_length']}\")\n",
        "\n",
        "    print(f\"\\nAnalysis files saved to {output_dir}:\")\n",
        "    print(f\"- Token length distribution: {length_plot_path}\")\n",
        "    print(f\"- Vocabulary CSV: {vocab_csv_path}\")\n",
        "    print(f\"- Statistics: {stats_path}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def visualize_token_embeddings(model, tokenizer, output_dir=\"vocab_analysis\"):\n",
        "    \"\"\"\n",
        "    Visualize token embeddings using dimensionality reduction\n",
        "\n",
        "    Args:\n",
        "        model: Trained HindiTokenizerModel\n",
        "        tokenizer: Trained SimpleHindiTokenizer\n",
        "        output_dir: Directory to save visualizations\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the embedding visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from sklearn.manifold import TSNE\n",
        "        import numpy as np\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Get token embeddings from the model\n",
        "        embeddings = model.token_embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "        # Select a subset of tokens for visualization (e.g., first 500)\n",
        "        num_tokens = min(500, len(tokenizer.vocab))\n",
        "\n",
        "        # Apply t-SNE for dimensionality reduction\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        embeddings_2d = tsne.fit_transform(embeddings[:num_tokens])\n",
        "\n",
        "        # Create a DataFrame for plotting\n",
        "        tokens = [token for token, idx in sorted(tokenizer.vocab.items(), key=lambda x: x[1])[:num_tokens]]\n",
        "        embedding_df = pd.DataFrame({\n",
        "            'x': embeddings_2d[:, 0],\n",
        "            'y': embeddings_2d[:, 1],\n",
        "            'token': tokens\n",
        "        })\n",
        "\n",
        "        # Plot the embeddings\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.scatterplot(x='x', y='y', data=embedding_df)\n",
        "\n",
        "        # Add token labels for special tokens and a few regular tokens\n",
        "        for i, row in embedding_df.iterrows():\n",
        "            if i < 10 or i % 50 == 0:  # Label special tokens and some regular tokens\n",
        "                plt.text(row['x'], row['y'], row['token'], fontsize=9)\n",
        "\n",
        "        plt.title('t-SNE Visualization of Token Embeddings')\n",
        "        plt.xlabel('t-SNE Dimension 1')\n",
        "        plt.ylabel('t-SNE Dimension 2')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Save the plot\n",
        "        embedding_plot_path = os.path.join(output_dir, \"token_embeddings_tsne.png\")\n",
        "        plt.savefig(embedding_plot_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Token embeddings visualization saved to {embedding_plot_path}\")\n",
        "        return embedding_plot_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing token embeddings: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLaTGu6LHM4K"
      },
      "outputs": [],
      "source": [
        "# Load and analyze vocabulary\n",
        "tokenizer_file = \"model_output/hindi_tokenizer.json\"\n",
        "vocab = load_tokenizer_vocabulary(tokenizer_file)\n",
        "stats = analyze_vocabulary(vocab, output_dir=\"vocab_analysis\")\n",
        "\n",
        "# Visualize token embeddings\n",
        "embedding_plot = visualize_token_embeddings(model, tokenizer, output_dir=\"vocab_analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMnx4MrfHM4L"
      },
      "source": [
        "## 6. Testing the Tokenizer\n",
        "\n",
        "Let's test our trained tokenizer on some Hindi text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZFs-F6VHM4L"
      },
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "test_results = test_hindi_tokenizer(model, tokenizer)\n",
        "\n",
        "# Test with custom text\n",
        "custom_text = \"भारत एक विशाल देश है जिसमें अनेक भाषाएँ बोली जाती हैं।\"\n",
        "custom_results = test_hindi_tokenizer(model, tokenizer, test_text=custom_text)\n",
        "\n",
        "# Save test results\n",
        "with open(\"model_output/test_results.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        \"default_test\": test_results,\n",
        "        \"custom_test\": custom_results\n",
        "    }, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn9OogzAHM4L"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "1. Collected 10 Hindi Wikipedia pages for training data\n",
        "2. Preprocessed the Hindi text data to create a clean corpus\n",
        "3. Defined a transformer-based tokenizer model architecture with fixed attention mask handling\n",
        "4. Trained the model for 5 epochs\n",
        "5. Analyzed and visualized the learned vocabulary\n",
        "6. Tested the tokenizer on new Hindi text\n",
        "\n",
        "The trained tokenizer can now be used for various NLP tasks involving Hindi text. The model architecture includes multiple transformer layers with multi-head attention, making it a robust solution for tokenization tasks.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Fine-tune the model on specific domains or tasks\n",
        "- Integrate the tokenizer with other NLP models\n",
        "- Expand the training data to improve vocabulary coverage\n",
        "- Experiment with different model architectures and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFwvFMWV8NzI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Helper functions for Hindi text generation and probability visualization.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Assuming the model and tokenizer classes are defined elsewhere in the notebook\n",
        "# from fixed_hindi_tokenizer_colab import SimpleHindiTokenizer, HindiTokenizerModel\n",
        "\n",
        "def load_model_and_tokenizer(model_path, tokenizer_path, device):\n",
        "    \"\"\"\n",
        "    Load the trained Hindi tokenizer model and the tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint (.pt file).\n",
        "        tokenizer_path (str): Path to the saved tokenizer JSON file.\n",
        "        device (torch.device): Device to load the model onto (cpu or cuda).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
        "    tokenizer = SimpleHindiTokenizer()\n",
        "    tokenizer.load_tokenizer(tokenizer_path)\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "    print(f\"Loading model checkpoint from: {model_path}\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model checkpoint not found at {model_path}\")\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Infer model parameters from checkpoint if available\n",
        "    vocab_size = checkpoint.get(\"vocab_size\", len(tokenizer.vocab))\n",
        "    embed_dim = checkpoint.get(\"embed_dim\", 128) # Default if not found\n",
        "    num_heads = checkpoint.get(\"num_heads\", 4)   # Default if not found\n",
        "    num_layers = checkpoint.get(\"num_layers\", 4)  # Default if not found\n",
        "    ff_dim = checkpoint.get(\"ff_dim\", 512)     # Default if not found\n",
        "    # Assuming max_seq_length was consistent during training, e.g., 64\n",
        "    max_seq_length = checkpoint.get(\"max_seq_length\", 64)\n",
        "\n",
        "    print(\"Instantiating model...\")\n",
        "    model = HindiTokenizerModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        ff_dim=ff_dim,\n",
        "        max_seq_length=max_seq_length\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    print(\"Model loaded and set to evaluation mode.\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWDQLhyY8UNK"
      },
      "outputs": [],
      "source": [
        "def generate_hindi_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=50,\n",
        "    temperature=1.0,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate Hindi text from a prompt using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (HindiTokenizerModel): The trained language model.\n",
        "        tokenizer (SimpleHindiTokenizer): The trained tokenizer.\n",
        "        prompt (str): The starting Hindi text prompt.\n",
        "        max_length (int): Maximum number of tokens to generate after the prompt.\n",
        "        temperature (float): Controls randomness (higher = more random).\n",
        "        device (torch.device): Device to run generation on.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated Hindi text including the prompt.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    # Encode the prompt\n",
        "    encoded_prompt = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([encoded_prompt[\"ids\"]], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_token_ids = input_ids.tolist()[0] # Keep track of all generated IDs\n",
        "\n",
        "    print(f\"Starting generation from prompt: \", tokenizer.decode(generated_token_ids, skip_special_tokens=True))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get current sequence length\n",
        "            current_seq_length = input_ids.size(1)\n",
        "\n",
        "            # Prepare attention mask (all ones for generation)\n",
        "            attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "            # Get model logits for the last token\n",
        "            # Ensure input_ids and attention_mask are correctly shaped [batch_size, seq_len]\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Sample the next token ID\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Check if [SEP] token is generated\n",
        "            if next_token_id.item() == tokenizer.special_tokens[\"[SEP]\"]:\n",
        "                print(\"Generation stopped: [SEP] token encountered.\")\n",
        "                break\n",
        "\n",
        "            # Append the new token ID\n",
        "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "            generated_token_ids.append(next_token_id.item())\n",
        "\n",
        "            # Optional: Print the token being added\n",
        "            # print(f\"  -> Adding token: {tokenizer.decode([next_token_id.item()])}\")\n",
        "\n",
        "    # Decode the final sequence\n",
        "    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK1Kdit-Bgm2"
      },
      "outputs": [],
      "source": [
        "def generate_and_visualize_probabilities(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=10, # Keep visualization shorter\n",
        "    top_k=5,\n",
        "    temperature=1.0,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate Hindi text and visualize top-k token probabilities at each step.\n",
        "\n",
        "    Args:\n",
        "        model (HindiTokenizerModel): The trained language model.\n",
        "        tokenizer (SimpleHindiTokenizer): The trained tokenizer.\n",
        "        prompt (str): The starting Hindi text prompt.\n",
        "        max_length (int): Maximum number of tokens to generate.\n",
        "        top_k (int): Number of top probabilities to visualize.\n",
        "        temperature (float): Controls randomness.\n",
        "        device (torch.device): Device to run generation on.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated Hindi text including the prompt.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    # Encode the prompt\n",
        "    encoded_prompt = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([encoded_prompt[\"ids\"]], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_token_ids = input_ids.tolist()[0]\n",
        "\n",
        "    print(f\"Starting generation & visualization from prompt: \", tokenizer.decode(generated_token_ids, skip_special_tokens=True))\n",
        "    print(\"---\")\n",
        "\n",
        "    # Store visualization data\n",
        "    viz_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_length):\n",
        "            # Prepare attention mask\n",
        "            attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "            # Get model logits\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Get top-k probabilities and indices\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "\n",
        "            # Decode top-k tokens\n",
        "            top_k_tokens = [tokenizer.decode([idx.item()], skip_special_tokens=False) for idx in top_k_indices[0]]\n",
        "            top_k_probs_list = top_k_probs[0].cpu().numpy()\n",
        "\n",
        "            # Store data for this step\n",
        "            current_context = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "            viz_data.append({\n",
        "                \"step\": step + 1,\n",
        "                \"context\": current_context,\n",
        "                \"top_k_tokens\": top_k_tokens,\n",
        "                \"top_k_probs\": top_k_probs_list\n",
        "            })\n",
        "\n",
        "            # Sample the next token ID from the original distribution\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Check for [SEP] token\n",
        "            if next_token_id.item() == tokenizer.special_tokens[\"[SEP]\"]:\n",
        "                print(f\"Generation stopped at step {step+1}: [SEP] token encountered.\")\n",
        "                break\n",
        "\n",
        "            # Append the new token ID\n",
        "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "            generated_token_ids.append(next_token_id.item())\n",
        "\n",
        "    # Decode the final sequence\n",
        "    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "    print(\"\\nFinal Generated Text:\")\n",
        "    print(generated_text)\n",
        "    print(\"---\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    try:\n",
        "        plt.rcParams[\"font.family\"] = [\"Lohit Devanagari\", \"sans-serif\"]\n",
        "    except:\n",
        "        print(\"Warning: Lohit Devanagari font not found. Hindi text in plots might not render correctly.\")\n",
        "        plt.rcParams[\"font.family\"] = [\"sans-serif\"]\n",
        "\n",
        "    num_steps_to_plot = len(viz_data)\n",
        "    if num_steps_to_plot == 0:\n",
        "        print(\"No steps to visualize.\")\n",
        "        return generated_text\n",
        "\n",
        "    cols = 2\n",
        "    rows = (num_steps_to_plot + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(8 * cols, 4 * rows), squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, step_data in enumerate(viz_data):\n",
        "        ax = axes[i]\n",
        "        tokens = step_data[\"top_k_tokens\"]\n",
        "        probs = step_data[\"top_k_probs\"]\n",
        "        context = step_data[\"context\"]\n",
        "        step_num = step_data[\"step\"]\n",
        "\n",
        "        y_pos = np.arange(len(tokens))\n",
        "        # *** Corrected line below ***\n",
        "        ax.barh(y_pos, probs, align='center')\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(tokens)\n",
        "        ax.invert_yaxis()  # labels read top-to-bottom\n",
        "        ax.set_xlabel('Probability')\n",
        "        ax.set_title(f'Step {step_num}: Top {top_k} Predictions\\nContext: \"...{context[-30:]}\"')\n",
        "        ax.tick_params(axis='y', labelsize=10)\n",
        "        ax.grid(axis='x', linestyle=':', alpha=0.7)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HShM2THzB_x9"
      },
      "outputs": [],
      "source": [
        "# 1. Fix repetitive generation by implementing top-k sampling\n",
        "def generate_hindi_text(model, tokenizer, prompt, max_length=50, temperature=0.7, device='cpu', top_k=50):\n",
        "    \"\"\"\n",
        "    Generate Hindi text with top-k sampling to avoid repetition\n",
        "    \"\"\"\n",
        "    # Preprocess and tokenize prompt\n",
        "    clean_prompt = tokenizer.preprocess_hindi_text(prompt)\n",
        "    encoded = tokenizer.encode(clean_prompt, add_special_tokens=False)\n",
        "    input_ids = torch.tensor([encoded[\"ids\"]], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate continuation with top-k sampling\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        generated = input_ids\n",
        "        for _ in range(max_length):\n",
        "            # Get model predictions\n",
        "            logits = model(generated)[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            logits = logits / temperature\n",
        "            top_logits, top_indices = logits.topk(top_k, dim=-1)\n",
        "            probs = torch.softmax(top_logits, dim=-1)\n",
        "\n",
        "            # Sample from top-k tokens\n",
        "            next_token = top_indices.gather(-1, torch.multinomial(probs, 1))\n",
        "            generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "    # Decode and combine with prompt\n",
        "    generated_ids = generated[0].tolist()\n",
        "    continuation = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return prompt + continuation[len(clean_prompt):]\n",
        "\n",
        "# 2. Fix font warnings by installing Hindi fonts\n",
        "!sudo apt-get install fonts-lohit-devanagari -qq\n",
        "!sudo fc-cache -fv\n",
        "\n",
        "# 3. Update the visualization function to use installed fonts\n",
        "def generate_and_visualize_probabilities(model, tokenizer, prompt, max_length=10,\n",
        "                                        top_k=5, temperature=0.7, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate text with probability visualization using proper Hindi fonts\n",
        "    \"\"\"\n",
        "    # Set up matplotlib with Hindi font\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.font_manager as fm\n",
        "\n",
        "    # Find and use Hindi font\n",
        "    hindi_font = None\n",
        "    for font in fm.findSystemFonts():\n",
        "        if 'Lohit' in font or 'Devanagari' in font:\n",
        "            hindi_font = fm.FontProperties(fname=font)\n",
        "            break\n",
        "\n",
        "    if not hindi_font:\n",
        "        print(\"Hindi font not found, using default\")\n",
        "        hindi_font = None\n",
        "\n",
        "    # Preprocess and tokenize prompt\n",
        "    clean_prompt = tokenizer.preprocess_hindi_text(prompt)\n",
        "    encoded = tokenizer.encode(clean_prompt, add_special_tokens=False)\n",
        "    input_ids = torch.tensor([encoded[\"ids\"]], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate step by step\n",
        "    model.eval()\n",
        "    generated = input_ids\n",
        "    generated_text = prompt\n",
        "\n",
        "    for step in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            logits = model(generated)[:, -1, :]\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get top-k predictions\n",
        "        top_probs, top_indices = probs.topk(top_k, dim=-1)\n",
        "        top_tokens = [tokenizer.inverse_vocab.get(idx.item(), \"[UNK]\")\n",
        "                      for idx in top_indices[0]]\n",
        "\n",
        "        # Visualize probabilities\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.barh(top_tokens[::-1], top_probs[0].tolist()[::-1])\n",
        "        plt.title(f\"Step {step+1}: '{generated_text}'\")\n",
        "        plt.xlabel(\"Probability\")\n",
        "\n",
        "        if hindi_font:\n",
        "            for label in plt.gca().get_yticklabels():\n",
        "                label.set_fontproperties(hindi_font)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Sample next token\n",
        "        next_token = torch.multinomial(probs, 1)\n",
        "        generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "        # Update generated text\n",
        "        new_token = tokenizer.inverse_vocab.get(next_token[0,0].item(), \"\")\n",
        "        generated_text += \" \" + new_token if new_token else \"\"\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# 4. Example Usage Block (Updated)\n",
        "\n",
        "# Define paths (adjust if your output directory is different)\n",
        "MODEL_OUTPUT_DIR = \"model_output\"\n",
        "TOKENIZER_PATH = os.path.join(MODEL_OUTPUT_DIR, \"hindi_tokenizer.json\")\n",
        "MODEL_PATH = os.path.join(MODEL_OUTPUT_DIR, \"hindi_tokenizer_model_best.pt\") # Use the best saved model\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Initialize variables to None in case loading fails\n",
        "loaded_model = None\n",
        "loaded_tokenizer = None\n",
        "\n",
        "try:\n",
        "    # Ensure the model and tokenizer classes are defined\n",
        "    loaded_model, loaded_tokenizer = load_model_and_tokenizer(MODEL_PATH, TOKENIZER_PATH, DEVICE)\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}. Make sure necessary classes are defined.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Ensure files exist in {MODEL_OUTPUT_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Generate Text\n",
        "if loaded_model is not None and loaded_tokenizer is not None:\n",
        "    user_prompt = input(\"Enter your Hindi prompt (e.g., भारत एक): \") or \"भारत एक\"\n",
        "    print(f\"Using prompt: {user_prompt}\")\n",
        "\n",
        "    print(\"\\n--- Generating Text (Standard) ---\")\n",
        "    generated_output = generate_hindi_text(\n",
        "        model=loaded_model,\n",
        "        tokenizer=loaded_tokenizer,\n",
        "        prompt=user_prompt,\n",
        "        max_length=50,\n",
        "        temperature=0.7,\n",
        "        top_k=50,  # Add top-k sampling\n",
        "        device=DEVICE\n",
        "    )\n",
        "    print(\"\\nFinal Generated Text:\")\n",
        "    print(generated_output)\n",
        "\n",
        "    print(\"\\n--- Generating Text with Probability Visualization ---\")\n",
        "    viz_output = generate_and_visualize_probabilities(\n",
        "        model=loaded_model,\n",
        "        tokenizer=loaded_tokenizer,\n",
        "        prompt=user_prompt,\n",
        "        max_length=10,\n",
        "        top_k=5,\n",
        "        temperature=0.7,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    print(\"\\nFinal Generated Text with Visualization:\")\n",
        "    print(viz_output)\n",
        "else:\n",
        "    print(\"Skipping generation as model/tokenizer failed to load.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}